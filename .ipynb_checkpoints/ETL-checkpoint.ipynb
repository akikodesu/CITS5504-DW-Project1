{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decfa85-3f49-4bdd-bd26-d5b65269287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de2ce8-7edc-40fb-bf2b-2b7c005f5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_clean_cols(file_path, sheet_index=1, skip_rows=4):\n",
    "    \"\"\"\n",
    "    Load a specific sheet from an Excel file and standardize column names.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned and standardized column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(\n",
    "        file_path,\n",
    "        sheet_name=sheet_index,  # Read the specified sheet\n",
    "        skiprows=skip_rows,      # Skip non-data rows at the top\n",
    "        header=0                 # Use the first remaining row as column headers\n",
    "    )\n",
    "\n",
    "    # converting to lowercase, and replacing spaces with underscores\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.replace('\\n', '', regex=False)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(' ', '_', regex=False)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and clean the fatal crashes and fatalities datasets\n",
    "crashes_df = load_excel_clean_cols(\"bitre_fatal_crashes_dec2024.xlsx\")\n",
    "fatalities_df = load_excel_clean_cols(\"bitre_fatalities_dec2024.xlsx\")\n",
    "\n",
    "# Display basic information about the two DataFrames\n",
    "# (e.g., number of entries, column names, data types, non-null counts)\n",
    "crashes_df.info()\n",
    "fatalities_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbbe79-49a0-46ef-915d-feb0c92fbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## crashesè¡¨æ˜¯ä¸èƒ½æœ‰é‡å¤çš„ å»é‡\n",
    "duplicates = crashes_df.duplicated(subset=['crash_id'])\n",
    "print(\"é‡å¤è®°å½•æ•°ï¼š\", duplicates.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b387e-4fd4-431c-9e5f-025fe70e0fb2",
   "metadata": {},
   "source": [
    "#### æ ¹æ®project å¼•å…¥ç¬¬ä¸‰ä¸ªå…³è”æ•°æ® æˆ‘ä»¬é€‰æ‹©äº†ABSæ¥çš„ xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7e7d8-5b89-4797-bb42-b68dd7a5af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–åŸå§‹ Excelï¼ˆ2023 å¹´äººå£å¹´é¾„ç»“æ„ï¼‰\n",
    "age_profile_path = \"32350DS0004_2023.xlsx\"\n",
    "\n",
    "# Step 1ï¼šè¯»å–åŸå§‹æ•°æ®ï¼ˆå‰ 10 è¡Œç”¨äºæ£€æŸ¥ç»“æ„ï¼‰\n",
    "df_raw = pd.read_excel(age_profile_path, sheet_name=1,skiprows=4,header=0)\n",
    "\n",
    "# Step 1: æå–ç¬¬ 0 è¡Œä½œä¸ºå•ä½/è¡¥å……åˆ—åä¿¡æ¯\n",
    "unit_row = df_raw.iloc[0].fillna('').astype(str)\n",
    "\n",
    "# Step 2: ç°æœ‰åˆ—åï¼ˆåŸå§‹ï¼‰æ¥è‡ª header è¡Œ\n",
    "original_cols = df_raw.columns.astype(str)\n",
    "\n",
    "# Step 3: åˆå¹¶å•ä½ä¿¡æ¯ â†’ æ–°åˆ—å\n",
    "combined_cols = (\n",
    "    original_cols.str.strip() + ' ' + unit_row.str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# Step 4: æ ‡å‡†åŒ–åˆ—åï¼šå°å†™ã€ä¸‹åˆ’çº¿ã€å»ç‰¹æ®Šç¬¦å·\n",
    "cleaned_cols = (\n",
    "    combined_cols\n",
    "    .str.replace(r'\\s+', '_', regex=True)\n",
    "    .str.replace(r'[^\\w_]', '', regex=True)\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Step 5: åº”ç”¨æ–°åˆ—åï¼Œå¹¶å»é™¤å•ä½è¡Œ\n",
    "age_df = df_raw.iloc[1:].copy()\n",
    "age_df.columns = cleaned_cols\n",
    "\n",
    "# Step 6: é‡å‘½åå…³é”®åˆ—\n",
    "age_df = age_df.rename(columns={\n",
    "    'unnamed_1_st_name': 'state',\n",
    "    'unnamed_2_lga_code': 'lga_code',\n",
    "    'unnamed_3_lga_name': 'lga_name',\n",
    "    'median_age_years': 'median_age',\n",
    "    'people_aged_65_years_and_over_': 'pct_65_plus'\n",
    "})\n",
    "\n",
    "# å±•ç¤ºæ¸…æ´—åçš„è¡¨ç»“æ„\n",
    "age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ec30a-5490-46dc-a33f-7129bced9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å”¯ä¸€å€¼åˆ†æ\n",
    "def unique_value_report(df, max_display=20):\n",
    "    \"\"\"\n",
    "    Print unique value summary for each column in a DataFrame.\n",
    "    If the column has fewer than `max_display` unique values, print value counts.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        nunique = df[col].nunique(dropna=False)\n",
    "        print(f\"ğŸ”¹ Column: {col}\")\n",
    "        print(f\"   â¤ Unique values: {nunique}\")\n",
    "        \n",
    "        if nunique <= max_display:\n",
    "            print(df[col].value_counts(dropna=False))\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f418f-061b-4159-b1a3-3b78c9a42e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------CRASHES---------------\")\n",
    "unique_value_report(crashes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772409ad-504e-43a6-be45-e2727df99cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------FATALITIES---------------\")\n",
    "unique_value_report(fatalities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16acdb1-0796-4d35-bc3b-074d050dcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total unique crash_id in crashes_df: {crashes_df['crash_id'].nunique()}\")\n",
    "print(f\"Total unique crash_id in fatalities_df: {fatalities_df['crash_id'].nunique()}\")\n",
    "print(f\"Total rows in fatalities_df: {len(fatalities_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94605a1-97c2-48a6-8ca0-87531c1ec65a",
   "metadata": {},
   "source": [
    "#### è°ƒæŸ¥\n",
    "é€šè¿‡æ•°æ®çš„åŸè¡¨æ¥æºå’Œå’Œä»¥ä¸Šè„šæœ¬å”¯ä¸€å€¼è°ƒæŸ¥,æˆ‘ä»¬å¯ä»¥å‘ç°,crashes_df å’Œ fatalities_df æ˜¯é€šè¿‡crash_idè¿›è¡Œå…³è”çš„.\n",
    "æˆ‘ä»¬å‘ç°crashes_df å’Œ fatalities_dfä¸­çš„sa4_name_2021å’Œnational_lga_name_2021æœ‰å¤§é‡null,è€Œä¸”national_remoteness_areasæœ‰å¤§é‡çš„unknown *\n",
    "æˆ‘ä»¬å…ˆè®¡ç®—ä¸‹è¿™ç§ç¼ºå¤±å€¼åˆ°åº•å å¤šå°‘ç™¾åˆ†æ¯”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f2813-06df-462e-b9db-9e2572ea120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing ratio for geo fields\n",
    "missing_rates = crashes_df[['national_lga_name_2021', 'sa4_name_2021']].isna().mean() * 100\n",
    "print(\"Missing Value Percentage:\")\n",
    "print(missing_rates)\n",
    "\n",
    "# Count 'Unknown' in national_remoteness_areas\n",
    "unknown_count = (crashes_df['national_remoteness_areas'] == 'Unknown').sum()\n",
    "total_rows = len(crashes_df)\n",
    "unknown_ratio = unknown_count / total_rows * 100\n",
    "print(f\"\\n'Unknown' in national_remoteness_areas: {unknown_count} ({unknown_ratio:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36542d4e-809d-45aa-9116-c21b6c900c4e",
   "metadata": {},
   "source": [
    "### Assessing Missing Data in Geographic Fields\n",
    "\n",
    "To evaluate whether geographic fields should be retained or excluded from the dataset, we applied a quantitative scoring method based on missingness, uniqueness, and correlation with a key target variable (`age`). The goal is to identify fields that offer high analytical value while minimizing noise and sparsity.\n",
    "\n",
    "We first cleaned the data by replacing known placeholders such as `'Unknown'` and `-9` with `NaN`, across both categorical and numerical fields. This ensured consistent handling of missing or invalid values prior to analysis.\n",
    "\n",
    "For each field, we calculated:\n",
    "- **Missing Rate**: proportion of missing values\n",
    "- **Uniqueness Score**: the number of unique values normalized by dataset size\n",
    "- **Correlation with Target**: absolute correlation with the target variable (`age`), where applicable\n",
    "\n",
    "A composite **Total Score** was then computed by weighting these factors (50% missingness, 20% uniqueness, 30% correlation). Fields with higher scores are considered more reliable and analytically valuable.\n",
    "\n",
    "The resulting ranking allows us to compare all fields fairly, including the three geographic fields of interest: `national_lga_name_2021`, `sa4_name_2021`, and `national_remoteness_areas`. By examining their scores in relation to other variables, we can make a more informed decision on whether to retain or remove them from the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ced37-7dcb-4ef5-81dd-800c22bd48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'age'\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªä¸´æ—¶DataFrameè¿›è¡Œè¯„ä¼°ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®\n",
    "temp_df = fatalities_df.copy()\n",
    "\n",
    "# å¤„ç†ç‰¹æ®Šå€¼ï¼šå°†\"Unknown\"è½¬æ¢ä¸ºNaN\n",
    "special_values = {\n",
    "    'national_remoteness_areas': ['Unknown'],\n",
    "    'day_of_week': ['Unknown'],\n",
    "    'time_of_day': ['Unknown'],\n",
    "    # å¯ä»¥æ·»åŠ å…¶ä»–åŒ…å«ç‰¹æ®Šå€¼çš„å­—æ®µ\n",
    "}\n",
    "\n",
    "for field, values in special_values.items():\n",
    "    if field in temp_df.columns:\n",
    "        for val in values:\n",
    "            temp_df.loc[temp_df[field] == val, field] = np.nan\n",
    "\n",
    "# å¤„ç†-9å€¼\n",
    "neg9_fields = ['bus_involvement', 'heavy_rigid_truck_involvement', \n",
    "               'articulated_truck_involvement', 'speed_limit', 'gender', 'age', 'age_group']\n",
    "for field in neg9_fields:\n",
    "    if field in temp_df.columns:\n",
    "        temp_df.loc[(temp_df[field] == -9) | (temp_df[field] == '-9'), field] = np.nan\n",
    "\n",
    "# è®¡ç®—è°ƒæ•´åçš„ç¼ºå¤±ç‡\n",
    "numerical_cols = temp_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# åˆ›å»ºè¯„åˆ†DataFrame - å°†Fieldä½œä¸ºç´¢å¼•è€Œéåˆ—\n",
    "field_scores = pd.DataFrame({\n",
    "    'Missing_Rate': temp_df.isna().mean(),\n",
    "    'Unique_Values': temp_df.nunique() / len(temp_df),\n",
    "})\n",
    "\n",
    "correlations = {}\n",
    "for col in temp_df.columns:\n",
    "    if col != target_variable and col in numerical_cols:\n",
    "        try:\n",
    "            corr = abs(temp_df[col].corr(temp_df[target_variable]))\n",
    "            correlations[col] = corr\n",
    "        except:\n",
    "            correlations[col] = 0\n",
    "    else:\n",
    "        correlations[col] = 0\n",
    "\n",
    "field_scores['Correlation_With_Target'] = pd.Series(correlations)\n",
    "\n",
    "# è®¡ç®—æ€»åˆ† (ä½ç¼ºå¤±ç‡é«˜ï¼Œå”¯ä¸€å€¼é€‚ä¸­é«˜ï¼Œç›¸å…³æ€§é«˜ä¸ºä½³)\n",
    "field_scores['Total_Score'] = (\n",
    "    (1 - field_scores['Missing_Rate']) * 0.5 + \n",
    "    field_scores['Unique_Values'] * 0.2 +\n",
    "    field_scores['Correlation_With_Target'] * 0.3\n",
    ")\n",
    "\n",
    "# æ’åº\n",
    "field_scores = field_scores.sort_values('Total_Score', ascending=False)\n",
    "top_fields = field_scores.sort_values('Total_Score', ascending=False).head(25)\n",
    "\n",
    "# ç»˜åˆ¶æ°´å¹³æŸ±çŠ¶å›¾\n",
    "top_fields['Total_Score'].sort_values().plot(\n",
    "    kind='barh', \n",
    "    figsize=(10, 6), \n",
    "    title='Top 10 Fields by Total Score',\n",
    "    xlabel='Total Score',\n",
    "    color='skyblue',\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "geo_fields = ['national_lga_name_2021', 'sa4_name_2021', 'national_remoteness_areas']\n",
    "geo_scores = field_scores.loc[geo_fields]\n",
    "print(geo_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495e194-0371-4973-b9f0-cb8c7be8719f",
   "metadata": {},
   "source": [
    "### Decision on Dropping Geographic Fields\n",
    "\n",
    "To evaluate whether geographic fields should be retained in the dataset, we applied a multi-criteria scoring approach that considers missing rate, uniqueness, and correlation with a key outcome variable (`age`). This method assigns a composite score to each field, allowing us to assess its analytical value in a structured and consistent way.\n",
    "\n",
    "The resulting scores for the three geographic fields were:\n",
    "\n",
    "- `national_lga_name_2021`: Total Score = 0.113  \n",
    "- `sa4_name_2021`: Total Score = 0.112  \n",
    "- `national_remoteness_areas`: Total Score = 0.100  \n",
    "\n",
    "In contrast, most other fields in the dataset scored above 0.5, indicating significantly higher relevance and data quality.\n",
    "\n",
    "Given that:\n",
    "\n",
    "- These geographic fields have **very low total scores**, suggesting limited utility for predictive or descriptive analysis,\n",
    "- They exhibit **high proportions of missing or 'Unknown' values**, reducing their reliability,\n",
    "- They are **not directly aligned with the current analytical focus on victim-level characteristics**,\n",
    "\n",
    "We conclude that it is both justified and beneficial to **remove these fields** to simplify the dataset and improve overall signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246338d-84d4-4408-b4dc-91db8fe115f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 3 original geographic fields\n",
    "columns_to_drop = [\n",
    "    'national_lga_name_2021',\n",
    "    'sa4_name_2021',\n",
    "    'national_remoteness_areas'\n",
    "]\n",
    "\n",
    "# Final drop\n",
    "crashes_df.drop(columns=columns_to_drop,inplace=True)\n",
    "fatalities_df.drop(columns=columns_to_drop, inplace=True)\n",
    "crashes_df.info();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08859601-6373-4352-9715-3a9b45692510",
   "metadata": {},
   "source": [
    "# å¤–æ¥æ•°æ®ä¿ç•™é‡è¦å­—æ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48a606-23c7-4b6f-9ace-22250e49e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åªä¿ç•™åˆ†ææ‰€éœ€å­—æ®µ\n",
    "columns_to_keep = ['state', 'lga_code', 'lga_name', 'persons_no', 'median_age', 'pct_65_plus']\n",
    "age_df_cleaned = age_df[columns_to_keep].copy()\n",
    "\n",
    "# å±•ç¤ºç»“æœç¡®è®¤\n",
    "age_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289f2b9-adc8-4940-847b-274cfb321c18",
   "metadata": {},
   "source": [
    "### å¤„ç†-9/unknownç­‰éæ³•å­—æ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70f9e2-da63-440e-be03-0a36f009fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_neg9_columns(df: pd.DataFrame, df_name: str = \"DataFrame\") -> list:\n",
    "    \"\"\"\n",
    "    Detect which columns in a DataFrame contain the value -9 (int or str).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        df_name (str): Optional name of the DataFrame (for print info)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Columns containing -9\n",
    "    \"\"\"\n",
    "    columns_with_neg9 = []\n",
    "\n",
    "    print(f\"\\nğŸ” Scanning for '-9' in {df_name}...\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_str = df[col].astype(str).str.strip()\n",
    "        if (col_str == '-9').any():\n",
    "            count = (col_str == '-9').sum()\n",
    "            columns_with_neg9.append(col)\n",
    "            print(f\"âœ… Column '{col}' contains -9 â†’ {count} rows\")\n",
    "\n",
    "    if not columns_with_neg9:\n",
    "        print(\"âœ… No columns contain -9.\")\n",
    "    else:\n",
    "        print(f\"ğŸ“Œ Columns with '-9' in {df_name}: {columns_with_neg9}\")\n",
    "\n",
    "    return columns_with_neg9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79582028-b48e-48d4-9e09-157a5fa055c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_neg9_cols = detect_neg9_columns(crashes_df, df_name=\"crashes_df\")\n",
    "fatalities_neg9_cols = detect_neg9_columns(fatalities_df, df_name=\"fatalities_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc42cd-9a69-47b6-b6ba-040323fd6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°† -9 å’Œ \"-9\" ç±»å‹çš„å¼‚å¸¸å€¼ç»Ÿä¸€å¤„ç†ä¸º NaNï¼ˆé€‚ç”¨äº crashes å’Œ fatalitiesï¼‰\n",
    "for col in crashes_neg9_cols:\n",
    "    crashes_df.loc[crashes_df[col].isin(['-9', -9]), col] = np.nan\n",
    "\n",
    "for col in fatalities_neg9_cols:\n",
    "    fatalities_df.loc[fatalities_df[col].isin(['-9', -9]), col] = np.nan\n",
    "\n",
    "#road_userç‰¹æ®Šå¤„ç†\n",
    "mask = fatalities_df['road_user'].str.contains('Other/-9', na=False)\n",
    "fatalities_df.loc[mask, 'road_user'] = np.nan\n",
    "\n",
    "# å°† 'Unknown'ã€'unknown'ã€'UNKNOWN' ç­‰ç»Ÿä¸€å¤„ç†ä¸º NaN\n",
    "unknown_values = ['unknown', 'Unknown', 'UNKNOWN']\n",
    "\n",
    "crashes_df['day_of_week'] = crashes_df['day_of_week'].replace(unknown_values, np.nan)\n",
    "crashes_df['time_of_day'] = crashes_df['time_of_day'].replace(unknown_values, np.nan)\n",
    "fatalities_df['road_user'] = fatalities_df['road_user'].replace(unknown_values, np.nan)\n",
    "\n",
    "# å¤‡ä»½æ¸…æ´—åçš„ DataFrameï¼ˆä¿æŒä¸€è‡´ï¼‰\n",
    "crashes_df_cleaned = crashes_df.copy()\n",
    "fatalities_df_cleaned = fatalities_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcf5c4-7f16-4ddf-967b-3dd047d4cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¥éª¤ 1ï¸âƒ£ï¼šå®šä¹‰å…³é”®å­—æ®µ\n",
    "key_fields = [\n",
    "    'bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement',\n",
    "    'speed_limit', 'gender', 'age', 'age_group', 'road_user'\n",
    "]\n",
    "\n",
    "# æ­¥éª¤ 2ï¸âƒ£ï¼šè®¡ç®—æ¯è¡Œçš„ç¼ºå¤±æ•°é‡\n",
    "fatalities_df_cleaned['num_missing'] = fatalities_df_cleaned[key_fields].isna().sum(axis=1)\n",
    "\n",
    "# æ­¥éª¤ 3ï¸âƒ£ï¼šæ‰¾å‡ºç¼ºå¤±å­—æ®µ â‰¥ 4 çš„è®°å½•\n",
    "threshold = 4\n",
    "many_missing_rows = fatalities_df_cleaned[fatalities_df_cleaned['num_missing'] >= threshold]\n",
    "target_crash_ids = many_missing_rows['crash_id'].unique()\n",
    "\n",
    "print(f\"â— Found {len(many_missing_rows)} rows with â‰¥ {threshold} missing fields.\")\n",
    "print(f\"ğŸ†” Affected crash_id count: {len(target_crash_ids)}\")\n",
    "\n",
    "# æ­¥éª¤ 4ï¸âƒ£ï¼šä» crash_df_cleaned å’Œ fatalities_df_cleaned ä¸­åˆ é™¤è¿™äº› crash_id å¯¹åº”çš„è®°å½•\n",
    "before_crash = len(crashes_df_cleaned)\n",
    "crashes_df_cleaned = crashes_df_cleaned[~crashes_df_cleaned['crash_id'].isin(target_crash_ids)]\n",
    "after_crash = len(crashes_df_cleaned)\n",
    "print(\"ğŸ“‹ crash_id values to be deleted:\")\n",
    "print(target_crash_ids)\n",
    "\n",
    "before_fatalities = len(fatalities_df_cleaned)\n",
    "fatalities_df_cleaned = fatalities_df_cleaned[~fatalities_df_cleaned['crash_id'].isin(target_crash_ids)]\n",
    "after_fatalities = len(fatalities_df_cleaned)\n",
    "\n",
    "print(f\"âœ… Removed {before_crash - after_crash} rows from crashes_df_cleaned.\")\n",
    "print(f\"âœ… Removed {before_fatalities - after_fatalities} rows from fatalities_df_cleaned.\")\n",
    "\n",
    "# æ¸…é™¤è¾…åŠ©åˆ—\n",
    "fatalities_df_cleaned.drop(columns=['num_missing'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997064b-bc57-43a7-846f-1b1860a69d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fad4a-f6d5-422e-b955-01368534b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crashesæ•°æ®ç±»å‹è½¬æ¢\n",
    "categorical_cols = [\n",
    "    'state', 'dayweek', 'crash_type', 'day_of_week', 'time_of_day',\n",
    "    'national_road_type'\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    crashes_df_cleaned[col] = crashes_df_cleaned[col].astype('category')\n",
    "\n",
    "# Convert binary columns to boolean\n",
    "binary_cols = [\n",
    "    'bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement',\n",
    "    'christmas_period', 'easter_period'\n",
    "]\n",
    "\n",
    "for col in binary_cols:\n",
    "    crashes_df_cleaned[col] = (\n",
    "        crashes_df_cleaned[col].astype(str).str.lower().map({'yes': True, 'no': False}).astype('boolean')\n",
    "    )\n",
    "\n",
    "# Clean and convert speed_limit to numeric\n",
    "crashes_df_cleaned['speed_limit'] = crashes_df_cleaned['speed_limit'].replace('<40', 40)\n",
    "crashes_df_cleaned['speed_limit'] = (pd.to_numeric(crashes_df_cleaned['speed_limit'], errors='coerce').astype('Int64'))\n",
    "print(crashes_df_cleaned[['speed_limit','easter_period']].dtypes)\n",
    "crashes_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2359ca-d453-491d-b05d-76c48bae8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fatalitiesæ•°æ®ç±»å‹è½¬æ¢\n",
    "categorical_cols = ['road_user', 'gender', 'age_group']\n",
    "for col in categorical_cols:\n",
    "    fatalities_df_cleaned[col] = fatalities_df_cleaned[col].astype('category')\n",
    "\n",
    "# éªŒè¯è½¬æ¢ç»“æœ\n",
    "print(fatalities_df_cleaned[['road_user', 'gender','age_group']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a03be-2e1c-4029-8e62-afca7cd77ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ageæ•°æ®ç±»å‹è½¬æ¢\n",
    "age_df_cleaned['persons_no'] = pd.to_numeric(age_df_cleaned['persons_no'], errors='coerce')\n",
    "age_df_cleaned['median_age'] = pd.to_numeric(age_df_cleaned['median_age'], errors='coerce')\n",
    "age_df_cleaned['pct_65_plus'] = pd.to_numeric(age_df_cleaned['pct_65_plus'], errors='coerce')\n",
    "\n",
    "# å†æ¬¡å±•ç¤ºç¡®è®¤è½¬æ¢ç»“æœ\n",
    "age_df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951429e-19d9-4f1b-9afb-e85cab429a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ç‰¹å¾å·¥ç¨‹ roll up/ drill down\n",
    "# åˆ›å»ºå­£èŠ‚\n",
    "season_map = {12: 'Summer', 1: 'Summer', 2: 'Summer', \n",
    "              3: 'Autumn', 4: 'Autumn', 5: 'Autumn',\n",
    "              6: 'Winter', 7: 'Winter', 8: 'Winter', \n",
    "              9: 'Spring', 10: 'Spring', 11: 'Spring'}\n",
    "\n",
    "crashes_df_cleaned['season'] = crashes_df_cleaned['month'].map(season_map)\n",
    "crashes_df_cleaned['season'] = crashes_df_cleaned['season'].astype('category')\n",
    "\n",
    "\n",
    "hour_bin_labels = {\n",
    "    0: '00:00â€“02:00', 1: '02:00â€“04:00', 2: '04:00â€“06:00',\n",
    "    3: '06:00â€“08:00', 4: '08:00â€“10:00', 5: '10:00â€“12:00',\n",
    "    6: '12:00â€“14:00', 7: '14:00â€“16:00', 8: '16:00â€“18:00',\n",
    "    9: '18:00â€“20:00', 10: '20:00â€“22:00', 11: '22:00â€“00:00'\n",
    "}\n",
    "\n",
    "# ä» time å­—æ®µç›´æ¥æå–æ—¶æ®µæ ‡ç­¾ â†’ å†™å…¥ time_bin\n",
    "crashes_df_cleaned['time_bin'] = (\n",
    "    pd.to_numeric(\n",
    "        crashes_df_cleaned['time'].astype(str).str.extract(r'^(\\d{1,2})')[0],\n",
    "        errors='coerce'\n",
    "    ) // 2\n",
    ").map(hour_bin_labels).astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b89755-e3b1-4427-ab5c-00ce8c32e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c6162-ed6d-44c7-9a89-41bf6b1c8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f362795-5108-46da-9d54-33900c3cc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å·åç§°æ˜ å°„ä¸ºç¼©å†™\n",
    "state_abbrev_map = {\n",
    "    'New South Wales': 'NSW',\n",
    "    'Victoria': 'VIC',\n",
    "    'Queensland': 'QLD',\n",
    "    'South Australia': 'SA',\n",
    "    'Western Australia': 'WA',\n",
    "    'Tasmania': 'TAS',\n",
    "    'Northern Territory': 'NT',\n",
    "    'Australian Capital Territory': 'ACT',\n",
    "    'Other Territories': 'OT'\n",
    "}\n",
    "age_df_cleaned['state'] = age_df_cleaned['state'].replace(state_abbrev_map)\n",
    "\n",
    "# 2. èšåˆä¸ºå·çº§åˆ«åŠ æƒå¹³å‡ï¼Œå¹¶ä¿ç•™ä¸€ä½å°æ•°\n",
    "age_df_cleaned = age_df_cleaned.groupby('state').apply(\n",
    "    lambda g: pd.Series({\n",
    "        'total_population': g['persons_no'].sum(),\n",
    "        'weighted_median_age': round((g['median_age'] * g['persons_no']).sum() / g['persons_no'].sum(), 1),\n",
    "        'weighted_pct_65_plus': round((g['pct_65_plus'] * g['persons_no']).sum() / g['persons_no'].sum(), 1)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# 3. åŠ å…¥äººå£ç»“æ„ç­‰çº§æ ‡ç­¾ï¼ˆåŸºäºä¸­ä½å¹´é¾„ + 65+ æ¯”ä¾‹ï¼‰\n",
    "def classify_population_structure(median_age, pct_65):\n",
    "    if median_age < 30 and pct_65 < 10:\n",
    "        return 'Young society'\n",
    "    elif 30 <= median_age < 40 and 10 <= pct_65 < 14:\n",
    "        return 'Mature society'\n",
    "    elif 40 <= median_age <= 45 and 14 <= pct_65 < 22:\n",
    "        return 'Aging society'\n",
    "    elif median_age > 45 and pct_65 >= 22:\n",
    "        return 'Super-aged society'\n",
    "    else:\n",
    "        return 'Transitional society'\n",
    "        \n",
    "def classify_abs_age_group(pct):\n",
    "    if pct < 10.0:\n",
    "        return 'Less than 10%'\n",
    "    elif pct < 14.0:\n",
    "        return '10% to <14%'\n",
    "    elif pct < 18.0:\n",
    "        return '14% to <18%'\n",
    "    elif pct < 22.0:\n",
    "        return '18% to <22%'\n",
    "    else:\n",
    "        return '22% or more'\n",
    "        \n",
    "age_df_cleaned['population_structure_2023'] = age_df_cleaned.apply(\n",
    "    lambda row: classify_population_structure(row['weighted_median_age'], row['weighted_pct_65_plus']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "age_df_cleaned['abs_pct_65_plus_group_2023'] = age_df_cleaned['weighted_pct_65_plus'].apply(classify_abs_age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88e542-5034-4324-9d02-89341d4ba42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1506c-428f-4ee5-b43f-238a5877cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥number_fatalitiesä¸å®é™…fatalitiesè®°å½•çš„ä¸€è‡´æ€§\n",
    "fatality_counts = fatalities_df_cleaned.groupby('crash_id').size()\n",
    "crashes_with_mismatch = crashes_df_cleaned[\n",
    "    crashes_df_cleaned['crash_id'].isin(fatality_counts.index) & \n",
    "    (crashes_df_cleaned['number_fatalities'] != fatality_counts[crashes_df_cleaned['crash_id']].values)\n",
    "   ]\n",
    "print(f\"Crashes with fatality count mismatch: {len(crashes_with_mismatch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f90785-c915-42ee-ac4b-819e5b84e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: æ‰¾å‡ºé‡å¤åˆ—åï¼ˆä¸åŒ…æ‹¬ crash_idï¼‰\n",
    "common_columns = set(fatalities_df_cleaned.columns).intersection(crashes_df_cleaned.columns)\n",
    "common_columns.discard('crash_id')  # ä¿ç•™ä¸»é”®\n",
    "\n",
    "# Step 2: åˆ é™¤ fatalities ä¸­çš„é‡å¤åˆ—\n",
    "fatalities_reduced = fatalities_df_cleaned.drop(columns=common_columns)\n",
    "fatalities_reduced.head(5)\n",
    "\n",
    "# Step 3: åˆå¹¶ï¼Œä¸å†äº§ç”Ÿå†²çªåˆ—ï¼ˆä¹Ÿä¸éœ€è¦ suffixes å‚æ•°äº†ï¼‰\n",
    "fact_df = fatalities_reduced.merge(crashes_df_cleaned, on='crash_id', how='left')\n",
    "\n",
    "# âœ… âœ… æ’å…¥ä¸»é”® fatality_idï¼Œä» 1 å¼€å§‹ç¼–å·\n",
    "fact_df = fact_df.reset_index(drop=True)\n",
    "fact_df.insert(0, 'fatality_id', fact_df.index + 1)\n",
    "\n",
    "# æ ‡å‡†åŒ–å·åå¹¶åˆå¹¶å¹´é¾„æ•°æ®\n",
    "fact_df['state'] = fact_df['state'].str.upper()\n",
    "age_df_cleaned['state'] = age_df_cleaned['state'].str.upper()\n",
    "\n",
    "fact_df = fact_df.merge(\n",
    "    age_df_cleaned[['state','population_structure_2023', 'abs_pct_65_plus_group_2023']],\n",
    "    on='state',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a688bb-e0ef-46ed-921c-8fc8d748537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df.head()\n",
    "fact_df.to_csv('fatalities_mining_dataset.csv', index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4b7d2-addf-49c7-bc3c-10597be226d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf8bd5-33f2-4fa9-a280-1651d8543c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_dimension_key(fact_df, dim_df, dim_cols, key_name):\n",
    "    \"\"\"\n",
    "    æ›¿æ¢ fact_df ä¸­ç»´åº¦å­—æ®µä¸ºç»´åº¦ä¸»é”® key_nameï¼Œå¹¶åˆ é™¤åŸå­—æ®µ\n",
    "    \"\"\"\n",
    "    fact_df = fact_df.merge(dim_df, on=dim_cols, how='left')\n",
    "    fact_df = fact_df.drop(columns=dim_cols)\n",
    "    return fact_df\n",
    "\n",
    "# ======================== 1ï¸âƒ£ Dim_Time ========================\n",
    "dim_time = (\n",
    "    fact_df[['year', 'month', 'season']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(['year', 'month'])\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'time_id'})\n",
    ")\n",
    "dim_time['time_id'] += 1\n",
    "dim_time.to_csv('dim_time.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_time, ['year', 'month', 'season'], 'time_id')\n",
    "\n",
    "\n",
    "# ======================== 2ï¸âƒ£ Dim_Date ========================\n",
    "dim_date = (\n",
    "    fact_df[['dayweek', 'day_of_week']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'date_id'})\n",
    ")\n",
    "dim_date['date_id'] += 1\n",
    "dim_date.to_csv('dim_date.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_date, ['dayweek', 'day_of_week'], 'date_id')\n",
    "\n",
    "\n",
    "# ======================== 3ï¸âƒ£ Dim_DayNight ========================\n",
    "dim_daynight = (\n",
    "    fact_df[['time', 'time_bin', 'time_of_day']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'time_of_day_id'})\n",
    ")\n",
    "dim_daynight['time_of_day_id'] += 1\n",
    "dim_daynight.to_csv('dim_daynight.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_daynight, ['time', 'time_bin', 'time_of_day'], 'time_of_day_id')\n",
    "\n",
    "\n",
    "# ======================== 4ï¸âƒ£ Dim_VehicleInvl ========================\n",
    "dim_vehicle_invl = (\n",
    "    fact_df[['bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'vehicle_invl_id'})\n",
    ")\n",
    "dim_vehicle_invl['vehicle_invl_id'] += 1\n",
    "dim_vehicle_invl.to_csv('dim_vehicle_invl.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(\n",
    "    fact_df,\n",
    "    dim_vehicle_invl,\n",
    "    ['bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement'],\n",
    "    'vehicle_invl_id'\n",
    ")\n",
    "\n",
    "\n",
    "# ======================== 5ï¸âƒ£ Dim_Crash ========================\n",
    "dim_crash = (\n",
    "    fact_df[['crash_id', 'crash_type']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'crash_dim_id'})\n",
    ")\n",
    "dim_crash['crash_dim_id'] += 1\n",
    "dim_crash.to_csv('dim_crash.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_crash[['crash_id', 'crash_type', 'crash_dim_id']], ['crash_id', 'crash_type'], 'crash_dim_id')\n",
    "\n",
    "\n",
    "# ======================== 6ï¸âƒ£ Dim_Holiday ========================\n",
    "dim_holiday = (\n",
    "    fact_df[['christmas_period', 'easter_period']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'holiday_id'})\n",
    ")\n",
    "dim_holiday['holiday_id'] += 1\n",
    "dim_holiday.to_csv('dim_holiday.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_holiday, ['christmas_period', 'easter_period'], 'holiday_id')\n",
    "\n",
    "\n",
    "# ======================== 7ï¸âƒ£ Dim_Road ========================\n",
    "dim_road = (\n",
    "    fact_df[['speed_limit', 'national_road_type']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'road_id'})\n",
    ")\n",
    "dim_road['road_id'] += 1\n",
    "dim_road.to_csv('dim_road.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_road, ['speed_limit', 'national_road_type'], 'road_id')\n",
    "\n",
    "\n",
    "# ======================== 8ï¸âƒ£ Dim_State_Aging_Level ========================\n",
    "dim_state_aging_level = (\n",
    "    fact_df[['state', 'population_structure_2023', 'abs_pct_65_plus_group_2023']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'index': 'population_age_id',\n",
    "        'population_structure_2023': 'population_structure_2023',\n",
    "        'abs_pct_65_plus_group_2023': 'abs_pct_65_plus_group_2023'\n",
    "    })\n",
    ")\n",
    "dim_state_aging_level['population_age_id'] += 1\n",
    "dim_state_aging_level.to_csv('dim_state_aging_level.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(\n",
    "    fact_df,\n",
    "    dim_state_aging_level,\n",
    "    ['state', 'population_structure_2023', 'abs_pct_65_plus_group_2023'],\n",
    "    'population_age_id'\n",
    ")\n",
    "\n",
    "\n",
    "# ======================== 9ï¸âƒ£ Dim_Victim ========================\n",
    "dim_victim = (\n",
    "    fact_df[['gender', 'age_group', 'road_user']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'victim_type_id'})\n",
    ")\n",
    "dim_victim['victim_type_id'] += 1\n",
    "dim_victim.to_csv('dim_victim.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_victim, ['gender', 'age_group', 'road_user'], 'victim_type_id')\n",
    "\n",
    "# \n",
    "fact_df['is_young'] = fact_df['age'].between(18, 24, inclusive='both').astype(int)\n",
    "fact_df['is_general'] = fact_df['age'].between(25, 64, inclusive='both').astype(int)\n",
    "fact_df['is_senior'] = (fact_df['age'] >= 65).astype(int)\n",
    "fact_df = fact_df.drop(columns=['number_fatalities'])\n",
    "\n",
    "# æœ€ç»ˆå¯¼å‡º fact è¡¨\n",
    "fact_df.to_csv('fact_fatalities.csv', index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba0920-0519-461f-be17-076b38d7d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENDEND\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
