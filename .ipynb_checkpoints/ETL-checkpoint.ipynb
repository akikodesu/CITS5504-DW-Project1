{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decfa85-3f49-4bdd-bd26-d5b65269287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de2ce8-7edc-40fb-bf2b-2b7c005f5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_clean_cols(file_path, sheet_index=1, skip_rows=4):\n",
    "    \"\"\"\n",
    "    Load a specific sheet from an Excel file and standardize column names.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned and standardized column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(\n",
    "        file_path,\n",
    "        sheet_name=sheet_index,  # Read the specified sheet\n",
    "        skiprows=skip_rows,      # Skip non-data rows at the top\n",
    "        header=0                 # Use the first remaining row as column headers\n",
    "    )\n",
    "\n",
    "    # converting to lowercase, and replacing spaces with underscores\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.replace('\\n', '', regex=False)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(' ', '_', regex=False)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and clean the fatal crashes and fatalities datasets\n",
    "crashes_df = load_excel_clean_cols(\"bitre_fatal_crashes_dec2024.xlsx\")\n",
    "fatalities_df = load_excel_clean_cols(\"bitre_fatalities_dec2024.xlsx\")\n",
    "\n",
    "# Display basic information about the two DataFrames\n",
    "# (e.g., number of entries, column names, data types, non-null counts)\n",
    "crashes_df.info()\n",
    "fatalities_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbbe79-49a0-46ef-915d-feb0c92fbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## crashes表是不能有重复的 去重\n",
    "duplicates = crashes_df.duplicated(subset=['crash_id'])\n",
    "print(\"重复记录数：\", duplicates.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b387e-4fd4-431c-9e5f-025fe70e0fb2",
   "metadata": {},
   "source": [
    "#### 根据project 引入第三个关联数据 我们选择了ABS来的 xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7e7d8-5b89-4797-bb42-b68dd7a5af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取原始 Excel（2023 年人口年龄结构）\n",
    "age_profile_path = \"32350DS0004_2023.xlsx\"\n",
    "\n",
    "# Step 1：读取原始数据（前 10 行用于检查结构）\n",
    "df_raw = pd.read_excel(age_profile_path, sheet_name=1,skiprows=4,header=0)\n",
    "\n",
    "# Step 1: 提取第 0 行作为单位/补充列名信息\n",
    "unit_row = df_raw.iloc[0].fillna('').astype(str)\n",
    "\n",
    "# Step 2: 现有列名（原始）来自 header 行\n",
    "original_cols = df_raw.columns.astype(str)\n",
    "\n",
    "# Step 3: 合并单位信息 → 新列名\n",
    "combined_cols = (\n",
    "    original_cols.str.strip() + ' ' + unit_row.str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# Step 4: 标准化列名：小写、下划线、去特殊符号\n",
    "cleaned_cols = (\n",
    "    combined_cols\n",
    "    .str.replace(r'\\s+', '_', regex=True)\n",
    "    .str.replace(r'[^\\w_]', '', regex=True)\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Step 5: 应用新列名，并去除单位行\n",
    "age_df = df_raw.iloc[1:].copy()\n",
    "age_df.columns = cleaned_cols\n",
    "\n",
    "# Step 6: 重命名关键列\n",
    "age_df = age_df.rename(columns={\n",
    "    'unnamed_1_st_name': 'state',\n",
    "    'unnamed_2_lga_code': 'lga_code',\n",
    "    'unnamed_3_lga_name': 'lga_name',\n",
    "    'median_age_years': 'median_age',\n",
    "    'people_aged_65_years_and_over_': 'pct_65_plus'\n",
    "})\n",
    "\n",
    "# 展示清洗后的表结构\n",
    "age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ec30a-5490-46dc-a33f-7129bced9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 唯一值分析\n",
    "def unique_value_report(df, max_display=20):\n",
    "    \"\"\"\n",
    "    Print unique value summary for each column in a DataFrame.\n",
    "    If the column has fewer than `max_display` unique values, print value counts.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        nunique = df[col].nunique(dropna=False)\n",
    "        print(f\"🔹 Column: {col}\")\n",
    "        print(f\"   ➤ Unique values: {nunique}\")\n",
    "        \n",
    "        if nunique <= max_display:\n",
    "            print(df[col].value_counts(dropna=False))\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f418f-061b-4159-b1a3-3b78c9a42e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------CRASHES---------------\")\n",
    "unique_value_report(crashes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772409ad-504e-43a6-be45-e2727df99cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------FATALITIES---------------\")\n",
    "unique_value_report(fatalities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16acdb1-0796-4d35-bc3b-074d050dcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total unique crash_id in crashes_df: {crashes_df['crash_id'].nunique()}\")\n",
    "print(f\"Total unique crash_id in fatalities_df: {fatalities_df['crash_id'].nunique()}\")\n",
    "print(f\"Total rows in fatalities_df: {len(fatalities_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94605a1-97c2-48a6-8ca0-87531c1ec65a",
   "metadata": {},
   "source": [
    "#### 调查\n",
    "通过数据的原表来源和和以上脚本唯一值调查,我们可以发现,crashes_df 和 fatalities_df 是通过crash_id进行关联的.\n",
    "我们发现crashes_df 和 fatalities_df中的sa4_name_2021和national_lga_name_2021有大量null,而且national_remoteness_areas有大量的unknown *\n",
    "我们先计算下这种缺失值到底占多少百分比:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f2813-06df-462e-b9db-9e2572ea120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing ratio for geo fields\n",
    "missing_rates = crashes_df[['national_lga_name_2021', 'sa4_name_2021']].isna().mean() * 100\n",
    "print(\"Missing Value Percentage:\")\n",
    "print(missing_rates)\n",
    "\n",
    "# Count 'Unknown' in national_remoteness_areas\n",
    "unknown_count = (crashes_df['national_remoteness_areas'] == 'Unknown').sum()\n",
    "total_rows = len(crashes_df)\n",
    "unknown_ratio = unknown_count / total_rows * 100\n",
    "print(f\"\\n'Unknown' in national_remoteness_areas: {unknown_count} ({unknown_ratio:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36542d4e-809d-45aa-9116-c21b6c900c4e",
   "metadata": {},
   "source": [
    "### Assessing Missing Data in Geographic Fields\n",
    "\n",
    "To evaluate whether geographic fields should be retained or excluded from the dataset, we applied a quantitative scoring method based on missingness, uniqueness, and correlation with a key target variable (`age`). The goal is to identify fields that offer high analytical value while minimizing noise and sparsity.\n",
    "\n",
    "We first cleaned the data by replacing known placeholders such as `'Unknown'` and `-9` with `NaN`, across both categorical and numerical fields. This ensured consistent handling of missing or invalid values prior to analysis.\n",
    "\n",
    "For each field, we calculated:\n",
    "- **Missing Rate**: proportion of missing values\n",
    "- **Uniqueness Score**: the number of unique values normalized by dataset size\n",
    "- **Correlation with Target**: absolute correlation with the target variable (`age`), where applicable\n",
    "\n",
    "A composite **Total Score** was then computed by weighting these factors (50% missingness, 20% uniqueness, 30% correlation). Fields with higher scores are considered more reliable and analytically valuable.\n",
    "\n",
    "The resulting ranking allows us to compare all fields fairly, including the three geographic fields of interest: `national_lga_name_2021`, `sa4_name_2021`, and `national_remoteness_areas`. By examining their scores in relation to other variables, we can make a more informed decision on whether to retain or remove them from the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ced37-7dcb-4ef5-81dd-800c22bd48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'age'\n",
    "\n",
    "# 创建一个临时DataFrame进行评估，避免修改原始数据\n",
    "temp_df = fatalities_df.copy()\n",
    "\n",
    "# 处理特殊值：将\"Unknown\"转换为NaN\n",
    "special_values = {\n",
    "    'national_remoteness_areas': ['Unknown'],\n",
    "    'day_of_week': ['Unknown'],\n",
    "    'time_of_day': ['Unknown'],\n",
    "    # 可以添加其他包含特殊值的字段\n",
    "}\n",
    "\n",
    "for field, values in special_values.items():\n",
    "    if field in temp_df.columns:\n",
    "        for val in values:\n",
    "            temp_df.loc[temp_df[field] == val, field] = np.nan\n",
    "\n",
    "# 处理-9值\n",
    "neg9_fields = ['bus_involvement', 'heavy_rigid_truck_involvement', \n",
    "               'articulated_truck_involvement', 'speed_limit', 'gender', 'age', 'age_group']\n",
    "for field in neg9_fields:\n",
    "    if field in temp_df.columns:\n",
    "        temp_df.loc[(temp_df[field] == -9) | (temp_df[field] == '-9'), field] = np.nan\n",
    "\n",
    "# 计算调整后的缺失率\n",
    "numerical_cols = temp_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# 创建评分DataFrame - 将Field作为索引而非列\n",
    "field_scores = pd.DataFrame({\n",
    "    'Missing_Rate': temp_df.isna().mean(),\n",
    "    'Unique_Values': temp_df.nunique() / len(temp_df),\n",
    "})\n",
    "\n",
    "correlations = {}\n",
    "for col in temp_df.columns:\n",
    "    if col != target_variable and col in numerical_cols:\n",
    "        try:\n",
    "            corr = abs(temp_df[col].corr(temp_df[target_variable]))\n",
    "            correlations[col] = corr\n",
    "        except:\n",
    "            correlations[col] = 0\n",
    "    else:\n",
    "        correlations[col] = 0\n",
    "\n",
    "field_scores['Correlation_With_Target'] = pd.Series(correlations)\n",
    "\n",
    "# 计算总分 (低缺失率高，唯一值适中高，相关性高为佳)\n",
    "field_scores['Total_Score'] = (\n",
    "    (1 - field_scores['Missing_Rate']) * 0.5 + \n",
    "    field_scores['Unique_Values'] * 0.2 +\n",
    "    field_scores['Correlation_With_Target'] * 0.3\n",
    ")\n",
    "\n",
    "# 排序\n",
    "field_scores = field_scores.sort_values('Total_Score', ascending=False)\n",
    "top_fields = field_scores.sort_values('Total_Score', ascending=False).head(25)\n",
    "\n",
    "# 绘制水平柱状图\n",
    "top_fields['Total_Score'].sort_values().plot(\n",
    "    kind='barh', \n",
    "    figsize=(10, 6), \n",
    "    title='Top 10 Fields by Total Score',\n",
    "    xlabel='Total Score',\n",
    "    color='skyblue',\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "geo_fields = ['national_lga_name_2021', 'sa4_name_2021', 'national_remoteness_areas']\n",
    "geo_scores = field_scores.loc[geo_fields]\n",
    "print(geo_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495e194-0371-4973-b9f0-cb8c7be8719f",
   "metadata": {},
   "source": [
    "### Decision on Dropping Geographic Fields\n",
    "\n",
    "To evaluate whether geographic fields should be retained in the dataset, we applied a multi-criteria scoring approach that considers missing rate, uniqueness, and correlation with a key outcome variable (`age`). This method assigns a composite score to each field, allowing us to assess its analytical value in a structured and consistent way.\n",
    "\n",
    "The resulting scores for the three geographic fields were:\n",
    "\n",
    "- `national_lga_name_2021`: Total Score = 0.113  \n",
    "- `sa4_name_2021`: Total Score = 0.112  \n",
    "- `national_remoteness_areas`: Total Score = 0.100  \n",
    "\n",
    "In contrast, most other fields in the dataset scored above 0.5, indicating significantly higher relevance and data quality.\n",
    "\n",
    "Given that:\n",
    "\n",
    "- These geographic fields have **very low total scores**, suggesting limited utility for predictive or descriptive analysis,\n",
    "- They exhibit **high proportions of missing or 'Unknown' values**, reducing their reliability,\n",
    "- They are **not directly aligned with the current analytical focus on victim-level characteristics**,\n",
    "\n",
    "We conclude that it is both justified and beneficial to **remove these fields** to simplify the dataset and improve overall signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246338d-84d4-4408-b4dc-91db8fe115f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 3 original geographic fields\n",
    "columns_to_drop = [\n",
    "    'national_lga_name_2021',\n",
    "    'sa4_name_2021',\n",
    "    'national_remoteness_areas'\n",
    "]\n",
    "\n",
    "# Final drop\n",
    "crashes_df.drop(columns=columns_to_drop,inplace=True)\n",
    "fatalities_df.drop(columns=columns_to_drop, inplace=True)\n",
    "crashes_df.info();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08859601-6373-4352-9715-3a9b45692510",
   "metadata": {},
   "source": [
    "# 外来数据保留重要字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48a606-23c7-4b6f-9ace-22250e49e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只保留分析所需字段\n",
    "columns_to_keep = ['state', 'lga_code', 'lga_name', 'persons_no', 'median_age', 'pct_65_plus']\n",
    "age_df_cleaned = age_df[columns_to_keep].copy()\n",
    "\n",
    "# 展示结果确认\n",
    "age_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289f2b9-adc8-4940-847b-274cfb321c18",
   "metadata": {},
   "source": [
    "### 处理-9/unknown等非法字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70f9e2-da63-440e-be03-0a36f009fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_neg9_columns(df: pd.DataFrame, df_name: str = \"DataFrame\") -> list:\n",
    "    \"\"\"\n",
    "    Detect which columns in a DataFrame contain the value -9 (int or str).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        df_name (str): Optional name of the DataFrame (for print info)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Columns containing -9\n",
    "    \"\"\"\n",
    "    columns_with_neg9 = []\n",
    "\n",
    "    print(f\"\\n🔍 Scanning for '-9' in {df_name}...\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_str = df[col].astype(str).str.strip()\n",
    "        if (col_str == '-9').any():\n",
    "            count = (col_str == '-9').sum()\n",
    "            columns_with_neg9.append(col)\n",
    "            print(f\"✅ Column '{col}' contains -9 → {count} rows\")\n",
    "\n",
    "    if not columns_with_neg9:\n",
    "        print(\"✅ No columns contain -9.\")\n",
    "    else:\n",
    "        print(f\"📌 Columns with '-9' in {df_name}: {columns_with_neg9}\")\n",
    "\n",
    "    return columns_with_neg9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79582028-b48e-48d4-9e09-157a5fa055c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_neg9_cols = detect_neg9_columns(crashes_df, df_name=\"crashes_df\")\n",
    "fatalities_neg9_cols = detect_neg9_columns(fatalities_df, df_name=\"fatalities_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc42cd-9a69-47b6-b6ba-040323fd6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 -9 和 \"-9\" 类型的异常值统一处理为 NaN（适用于 crashes 和 fatalities）\n",
    "for col in crashes_neg9_cols:\n",
    "    crashes_df.loc[crashes_df[col].isin(['-9', -9]), col] = np.nan\n",
    "\n",
    "for col in fatalities_neg9_cols:\n",
    "    fatalities_df.loc[fatalities_df[col].isin(['-9', -9]), col] = np.nan\n",
    "\n",
    "#road_user特殊处理\n",
    "mask = fatalities_df['road_user'].str.contains('Other/-9', na=False)\n",
    "fatalities_df.loc[mask, 'road_user'] = np.nan\n",
    "\n",
    "# 将 'Unknown'、'unknown'、'UNKNOWN' 等统一处理为 NaN\n",
    "unknown_values = ['unknown', 'Unknown', 'UNKNOWN']\n",
    "\n",
    "crashes_df['day_of_week'] = crashes_df['day_of_week'].replace(unknown_values, np.nan)\n",
    "crashes_df['time_of_day'] = crashes_df['time_of_day'].replace(unknown_values, np.nan)\n",
    "fatalities_df['road_user'] = fatalities_df['road_user'].replace(unknown_values, np.nan)\n",
    "\n",
    "# 备份清洗后的 DataFrame（保持一致）\n",
    "crashes_df_cleaned = crashes_df.copy()\n",
    "fatalities_df_cleaned = fatalities_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcf5c4-7f16-4ddf-967b-3dd047d4cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 1️⃣：定义关键字段\n",
    "key_fields = [\n",
    "    'bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement',\n",
    "    'speed_limit', 'gender', 'age', 'age_group', 'road_user'\n",
    "]\n",
    "\n",
    "# 步骤 2️⃣：计算每行的缺失数量\n",
    "fatalities_df_cleaned['num_missing'] = fatalities_df_cleaned[key_fields].isna().sum(axis=1)\n",
    "\n",
    "# 步骤 3️⃣：找出缺失字段 ≥ 4 的记录\n",
    "threshold = 4\n",
    "many_missing_rows = fatalities_df_cleaned[fatalities_df_cleaned['num_missing'] >= threshold]\n",
    "target_crash_ids = many_missing_rows['crash_id'].unique()\n",
    "\n",
    "print(f\"❗ Found {len(many_missing_rows)} rows with ≥ {threshold} missing fields.\")\n",
    "print(f\"🆔 Affected crash_id count: {len(target_crash_ids)}\")\n",
    "\n",
    "# 步骤 4️⃣：从 crash_df_cleaned 和 fatalities_df_cleaned 中删除这些 crash_id 对应的记录\n",
    "before_crash = len(crashes_df_cleaned)\n",
    "crashes_df_cleaned = crashes_df_cleaned[~crashes_df_cleaned['crash_id'].isin(target_crash_ids)]\n",
    "after_crash = len(crashes_df_cleaned)\n",
    "print(\"📋 crash_id values to be deleted:\")\n",
    "print(target_crash_ids)\n",
    "\n",
    "before_fatalities = len(fatalities_df_cleaned)\n",
    "fatalities_df_cleaned = fatalities_df_cleaned[~fatalities_df_cleaned['crash_id'].isin(target_crash_ids)]\n",
    "after_fatalities = len(fatalities_df_cleaned)\n",
    "\n",
    "print(f\"✅ Removed {before_crash - after_crash} rows from crashes_df_cleaned.\")\n",
    "print(f\"✅ Removed {before_fatalities - after_fatalities} rows from fatalities_df_cleaned.\")\n",
    "\n",
    "# 清除辅助列\n",
    "fatalities_df_cleaned.drop(columns=['num_missing'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997064b-bc57-43a7-846f-1b1860a69d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fad4a-f6d5-422e-b955-01368534b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crashes数据类型转换\n",
    "categorical_cols = [\n",
    "    'state', 'dayweek', 'crash_type', 'day_of_week', 'time_of_day',\n",
    "    'national_road_type'\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    crashes_df_cleaned[col] = crashes_df_cleaned[col].astype('category')\n",
    "\n",
    "# Convert binary columns to boolean\n",
    "binary_cols = [\n",
    "    'bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement',\n",
    "    'christmas_period', 'easter_period'\n",
    "]\n",
    "\n",
    "for col in binary_cols:\n",
    "    crashes_df_cleaned[col] = (\n",
    "        crashes_df_cleaned[col].astype(str).str.lower().map({'yes': True, 'no': False}).astype('boolean')\n",
    "    )\n",
    "\n",
    "# Clean and convert speed_limit to numeric\n",
    "crashes_df_cleaned['speed_limit'] = crashes_df_cleaned['speed_limit'].replace('<40', 40)\n",
    "crashes_df_cleaned['speed_limit'] = (pd.to_numeric(crashes_df_cleaned['speed_limit'], errors='coerce').astype('Int64'))\n",
    "print(crashes_df_cleaned[['speed_limit','easter_period']].dtypes)\n",
    "crashes_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2359ca-d453-491d-b05d-76c48bae8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fatalities数据类型转换\n",
    "categorical_cols = ['road_user', 'gender', 'age_group']\n",
    "for col in categorical_cols:\n",
    "    fatalities_df_cleaned[col] = fatalities_df_cleaned[col].astype('category')\n",
    "\n",
    "# 验证转换结果\n",
    "print(fatalities_df_cleaned[['road_user', 'gender','age_group']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a03be-2e1c-4029-8e62-afca7cd77ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age数据类型转换\n",
    "age_df_cleaned['persons_no'] = pd.to_numeric(age_df_cleaned['persons_no'], errors='coerce')\n",
    "age_df_cleaned['median_age'] = pd.to_numeric(age_df_cleaned['median_age'], errors='coerce')\n",
    "age_df_cleaned['pct_65_plus'] = pd.to_numeric(age_df_cleaned['pct_65_plus'], errors='coerce')\n",
    "\n",
    "# 再次展示确认转换结果\n",
    "age_df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951429e-19d9-4f1b-9afb-e85cab429a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 特征工程 roll up/ drill down\n",
    "# 创建季节\n",
    "season_map = {12: 'Summer', 1: 'Summer', 2: 'Summer', \n",
    "              3: 'Autumn', 4: 'Autumn', 5: 'Autumn',\n",
    "              6: 'Winter', 7: 'Winter', 8: 'Winter', \n",
    "              9: 'Spring', 10: 'Spring', 11: 'Spring'}\n",
    "\n",
    "crashes_df_cleaned['season'] = crashes_df_cleaned['month'].map(season_map)\n",
    "crashes_df_cleaned['season'] = crashes_df_cleaned['season'].astype('category')\n",
    "\n",
    "\n",
    "hour_bin_labels = {\n",
    "    0: '00:00–02:00', 1: '02:00–04:00', 2: '04:00–06:00',\n",
    "    3: '06:00–08:00', 4: '08:00–10:00', 5: '10:00–12:00',\n",
    "    6: '12:00–14:00', 7: '14:00–16:00', 8: '16:00–18:00',\n",
    "    9: '18:00–20:00', 10: '20:00–22:00', 11: '22:00–00:00'\n",
    "}\n",
    "\n",
    "# 从 time 字段直接提取时段标签 → 写入 time_bin\n",
    "crashes_df_cleaned['time_bin'] = (\n",
    "    pd.to_numeric(\n",
    "        crashes_df_cleaned['time'].astype(str).str.extract(r'^(\\d{1,2})')[0],\n",
    "        errors='coerce'\n",
    "    ) // 2\n",
    ").map(hour_bin_labels).astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b89755-e3b1-4427-ab5c-00ce8c32e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c6162-ed6d-44c7-9a89-41bf6b1c8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f362795-5108-46da-9d54-33900c3cc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 州名称映射为缩写\n",
    "state_abbrev_map = {\n",
    "    'New South Wales': 'NSW',\n",
    "    'Victoria': 'VIC',\n",
    "    'Queensland': 'QLD',\n",
    "    'South Australia': 'SA',\n",
    "    'Western Australia': 'WA',\n",
    "    'Tasmania': 'TAS',\n",
    "    'Northern Territory': 'NT',\n",
    "    'Australian Capital Territory': 'ACT',\n",
    "    'Other Territories': 'OT'\n",
    "}\n",
    "age_df_cleaned['state'] = age_df_cleaned['state'].replace(state_abbrev_map)\n",
    "\n",
    "# 2. 聚合为州级别加权平均，并保留一位小数\n",
    "age_df_cleaned = age_df_cleaned.groupby('state').apply(\n",
    "    lambda g: pd.Series({\n",
    "        'total_population': g['persons_no'].sum(),\n",
    "        'weighted_median_age': round((g['median_age'] * g['persons_no']).sum() / g['persons_no'].sum(), 1),\n",
    "        'weighted_pct_65_plus': round((g['pct_65_plus'] * g['persons_no']).sum() / g['persons_no'].sum(), 1)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# 3. 加入人口结构等级标签（基于中位年龄 + 65+ 比例）\n",
    "def classify_population_structure(median_age, pct_65):\n",
    "    if median_age < 30 and pct_65 < 10:\n",
    "        return 'Young society'\n",
    "    elif 30 <= median_age < 40 and 10 <= pct_65 < 14:\n",
    "        return 'Mature society'\n",
    "    elif 40 <= median_age <= 45 and 14 <= pct_65 < 22:\n",
    "        return 'Aging society'\n",
    "    elif median_age > 45 and pct_65 >= 22:\n",
    "        return 'Super-aged society'\n",
    "    else:\n",
    "        return 'Transitional society'\n",
    "        \n",
    "def classify_abs_age_group(pct):\n",
    "    if pct < 10.0:\n",
    "        return 'Less than 10%'\n",
    "    elif pct < 14.0:\n",
    "        return '10% to <14%'\n",
    "    elif pct < 18.0:\n",
    "        return '14% to <18%'\n",
    "    elif pct < 22.0:\n",
    "        return '18% to <22%'\n",
    "    else:\n",
    "        return '22% or more'\n",
    "        \n",
    "age_df_cleaned['population_structure_2023'] = age_df_cleaned.apply(\n",
    "    lambda row: classify_population_structure(row['weighted_median_age'], row['weighted_pct_65_plus']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "age_df_cleaned['abs_pct_65_plus_group_2023'] = age_df_cleaned['weighted_pct_65_plus'].apply(classify_abs_age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88e542-5034-4324-9d02-89341d4ba42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1506c-428f-4ee5-b43f-238a5877cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查number_fatalities与实际fatalities记录的一致性\n",
    "fatality_counts = fatalities_df_cleaned.groupby('crash_id').size()\n",
    "crashes_with_mismatch = crashes_df_cleaned[\n",
    "    crashes_df_cleaned['crash_id'].isin(fatality_counts.index) & \n",
    "    (crashes_df_cleaned['number_fatalities'] != fatality_counts[crashes_df_cleaned['crash_id']].values)\n",
    "   ]\n",
    "print(f\"Crashes with fatality count mismatch: {len(crashes_with_mismatch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f90785-c915-42ee-ac4b-819e5b84e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 找出重复列名（不包括 crash_id）\n",
    "common_columns = set(fatalities_df_cleaned.columns).intersection(crashes_df_cleaned.columns)\n",
    "common_columns.discard('crash_id')  # 保留主键\n",
    "\n",
    "# Step 2: 删除 fatalities 中的重复列\n",
    "fatalities_reduced = fatalities_df_cleaned.drop(columns=common_columns)\n",
    "fatalities_reduced.head(5)\n",
    "\n",
    "# Step 3: 合并，不再产生冲突列（也不需要 suffixes 参数了）\n",
    "fact_df = fatalities_reduced.merge(crashes_df_cleaned, on='crash_id', how='left')\n",
    "\n",
    "# ✅ ✅ 插入主键 fatality_id，从 1 开始编号\n",
    "fact_df = fact_df.reset_index(drop=True)\n",
    "fact_df.insert(0, 'fatality_id', fact_df.index + 1)\n",
    "\n",
    "# 标准化州名并合并年龄数据\n",
    "fact_df['state'] = fact_df['state'].str.upper()\n",
    "age_df_cleaned['state'] = age_df_cleaned['state'].str.upper()\n",
    "\n",
    "fact_df = fact_df.merge(\n",
    "    age_df_cleaned[['state','population_structure_2023', 'abs_pct_65_plus_group_2023']],\n",
    "    on='state',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a688bb-e0ef-46ed-921c-8fc8d748537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df.head()\n",
    "fact_df.to_csv('fatalities_mining_dataset.csv', index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4b7d2-addf-49c7-bc3c-10597be226d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf8bd5-33f2-4fa9-a280-1651d8543c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_dimension_key(fact_df, dim_df, dim_cols, key_name):\n",
    "    \"\"\"\n",
    "    替换 fact_df 中维度字段为维度主键 key_name，并删除原字段\n",
    "    \"\"\"\n",
    "    fact_df = fact_df.merge(dim_df, on=dim_cols, how='left')\n",
    "    fact_df = fact_df.drop(columns=dim_cols)\n",
    "    return fact_df\n",
    "\n",
    "# ======================== 1️⃣ Dim_Time ========================\n",
    "dim_time = (\n",
    "    fact_df[['year', 'month', 'season']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(['year', 'month'])\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'time_id'})\n",
    ")\n",
    "dim_time['time_id'] += 1\n",
    "dim_time.to_csv('dim_time.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_time, ['year', 'month', 'season'], 'time_id')\n",
    "\n",
    "\n",
    "# ======================== 2️⃣ Dim_Date ========================\n",
    "dim_date = (\n",
    "    fact_df[['dayweek', 'day_of_week']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'date_id'})\n",
    ")\n",
    "dim_date['date_id'] += 1\n",
    "dim_date.to_csv('dim_date.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_date, ['dayweek', 'day_of_week'], 'date_id')\n",
    "\n",
    "\n",
    "# ======================== 3️⃣ Dim_DayNight ========================\n",
    "dim_daynight = (\n",
    "    fact_df[['time', 'time_bin', 'time_of_day']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'time_of_day_id'})\n",
    ")\n",
    "dim_daynight['time_of_day_id'] += 1\n",
    "dim_daynight.to_csv('dim_daynight.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_daynight, ['time', 'time_bin', 'time_of_day'], 'time_of_day_id')\n",
    "\n",
    "\n",
    "# ======================== 4️⃣ Dim_VehicleInvl ========================\n",
    "dim_vehicle_invl = (\n",
    "    fact_df[['bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'vehicle_invl_id'})\n",
    ")\n",
    "dim_vehicle_invl['vehicle_invl_id'] += 1\n",
    "dim_vehicle_invl.to_csv('dim_vehicle_invl.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(\n",
    "    fact_df,\n",
    "    dim_vehicle_invl,\n",
    "    ['bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement'],\n",
    "    'vehicle_invl_id'\n",
    ")\n",
    "\n",
    "\n",
    "# ======================== 5️⃣ Dim_Crash ========================\n",
    "dim_crash = (\n",
    "    fact_df[['crash_id', 'crash_type']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'crash_dim_id'})\n",
    ")\n",
    "dim_crash['crash_dim_id'] += 1\n",
    "dim_crash.to_csv('dim_crash.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_crash[['crash_id', 'crash_type', 'crash_dim_id']], ['crash_id', 'crash_type'], 'crash_dim_id')\n",
    "\n",
    "\n",
    "# ======================== 6️⃣ Dim_Holiday ========================\n",
    "dim_holiday = (\n",
    "    fact_df[['christmas_period', 'easter_period']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'holiday_id'})\n",
    ")\n",
    "dim_holiday['holiday_id'] += 1\n",
    "dim_holiday.to_csv('dim_holiday.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_holiday, ['christmas_period', 'easter_period'], 'holiday_id')\n",
    "\n",
    "\n",
    "# ======================== 7️⃣ Dim_Road ========================\n",
    "dim_road = (\n",
    "    fact_df[['speed_limit', 'national_road_type']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'road_id'})\n",
    ")\n",
    "dim_road['road_id'] += 1\n",
    "dim_road.to_csv('dim_road.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_road, ['speed_limit', 'national_road_type'], 'road_id')\n",
    "\n",
    "\n",
    "# ======================== 8️⃣ Dim_State_Aging_Level ========================\n",
    "dim_state_aging_level = (\n",
    "    fact_df[['state', 'population_structure_2023', 'abs_pct_65_plus_group_2023']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'index': 'population_age_id',\n",
    "        'population_structure_2023': 'population_structure_2023',\n",
    "        'abs_pct_65_plus_group_2023': 'abs_pct_65_plus_group_2023'\n",
    "    })\n",
    ")\n",
    "dim_state_aging_level['population_age_id'] += 1\n",
    "dim_state_aging_level.to_csv('dim_state_aging_level.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(\n",
    "    fact_df,\n",
    "    dim_state_aging_level,\n",
    "    ['state', 'population_structure_2023', 'abs_pct_65_plus_group_2023'],\n",
    "    'population_age_id'\n",
    ")\n",
    "\n",
    "\n",
    "# ======================== 9️⃣ Dim_Victim ========================\n",
    "dim_victim = (\n",
    "    fact_df[['gender', 'age_group', 'road_user']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'victim_type_id'})\n",
    ")\n",
    "dim_victim['victim_type_id'] += 1\n",
    "dim_victim.to_csv('dim_victim.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_victim, ['gender', 'age_group', 'road_user'], 'victim_type_id')\n",
    "\n",
    "# \n",
    "fact_df['is_young'] = fact_df['age'].between(18, 24, inclusive='both').astype(int)\n",
    "fact_df['is_general'] = fact_df['age'].between(25, 64, inclusive='both').astype(int)\n",
    "fact_df['is_senior'] = (fact_df['age'] >= 65).astype(int)\n",
    "fact_df = fact_df.drop(columns=['number_fatalities'])\n",
    "\n",
    "# 最终导出 fact 表\n",
    "fact_df.to_csv('fact_fatalities.csv', index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba0920-0519-461f-be17-076b38d7d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENDEND\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
