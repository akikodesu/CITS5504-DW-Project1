{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decfa85-3f49-4bdd-bd26-d5b65269287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn\n",
    "%pip install openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8898a-a1ee-4d7f-b8d6-d9f821bf6f05",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de2ce8-7edc-40fb-bf2b-2b7c005f5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_clean_cols(file_path, sheet_index=1, skip_rows=4):\n",
    "    \"\"\"\n",
    "    Load a specific sheet from an Excel file and standardize column names.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned and standardized column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(\n",
    "        file_path,\n",
    "        sheet_name=sheet_index,  # Read the specified sheet\n",
    "        skiprows=skip_rows,      # Skip non-data rows at the top\n",
    "        header=0                 # Use the first remaining row as column headers\n",
    "    )\n",
    "\n",
    "    # converting to lowercase, and replacing spaces with underscores\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.replace('\\n', '', regex=False)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(' ', '_', regex=False)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and clean the fatal crashes and fatalities datasets\n",
    "crashes_df = load_excel_clean_cols(\"bitre_fatal_crashes_dec2024.xlsx\")\n",
    "fatalities_df = load_excel_clean_cols(\"bitre_fatalities_dec2024.xlsx\")\n",
    "\n",
    "# Display basic information about the two DataFrames\n",
    "# (e.g., number of entries, column names, data types, non-null counts)\n",
    "crashes_df.info()\n",
    "fatalities_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbbe79-49a0-46ef-915d-feb0c92fbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## # Check for duplicate crash IDs in the crashes dataframe\n",
    "duplicates = crashes_df.duplicated(subset=['crash_id'])\n",
    "print(\"duplicates numberÔºö\", duplicates.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b387e-4fd4-431c-9e5f-025fe70e0fb2",
   "metadata": {},
   "source": [
    "#### Loading Median age, sex ratio and broad age groups, by SA2 and above, 2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7e7d8-5b89-4797-bb42-b68dd7a5af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Australian Bureau of Statistics age profile dataset\n",
    "age_profile_path = \"32350DS0004_2023.xlsx\"\n",
    "\n",
    "# Load raw data, skipping header rows\n",
    "df_raw = pd.read_excel(age_profile_path, sheet_name=1, skiprows=4, header=0)\n",
    "\n",
    "# Extract unit information from the first row\n",
    "unit_row = df_raw.iloc[0].fillna('').astype(str)\n",
    "\n",
    "# Get original column names\n",
    "original_cols = df_raw.columns.astype(str)\n",
    "\n",
    "# Combine column names with their units\n",
    "combined_cols = (\n",
    "    original_cols.str.strip() + ' ' + unit_row.str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# Clean column names: replace spaces with underscores, remove special characters, convert to lowercase\n",
    "cleaned_cols = (\n",
    "    combined_cols\n",
    "    .str.replace(r'\\s+', '_', regex=True)\n",
    "    .str.replace(r'[^\\w_]', '', regex=True)\n",
    "    .str.lower()\n",
    ")\n",
    "# Create new dataframe excluding the unit row\n",
    "age_df = df_raw.iloc[1:].copy()\n",
    "age_df.columns = cleaned_cols\n",
    "\n",
    "# Rename specific columns for clarity and consistency\n",
    "age_df = age_df.rename(columns={\n",
    "    'unnamed_1_st_name': 'state',\n",
    "    'unnamed_2_lga_code': 'lga_code',\n",
    "    'unnamed_3_lga_name': 'lga_name',\n",
    "    'median_age_years': 'median_age',\n",
    "    'people_aged_65_years_and_over_': 'pct_65_plus'\n",
    "})\n",
    "\n",
    "# Display the first few rows of the processed dataframe\n",
    "age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ec30a-5490-46dc-a33f-7129bced9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_value_report(df, max_display=20):\n",
    "    \"\"\"\n",
    "    Print unique value summary for each column in a DataFrame.\n",
    "    If the column has fewer than `max_display` unique values, print value counts.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        nunique = df[col].nunique(dropna=False)\n",
    "        print(f\"‚û§ Column: {col}\")\n",
    "        print(f\"   ‚û§ Unique values: {nunique}\")\n",
    "        \n",
    "        if nunique <= max_display:\n",
    "            print(df[col].value_counts(dropna=False))\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f418f-061b-4159-b1a3-3b78c9a42e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------CRASHES---------------\")\n",
    "unique_value_report(crashes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772409ad-504e-43a6-be45-e2727df99cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------FATALITIES---------------\")\n",
    "unique_value_report(fatalities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16acdb1-0796-4d35-bc3b-074d050dcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total unique crash_id in crashes_df: {crashes_df['crash_id'].nunique()}\")\n",
    "print(f\"Total unique crash_id in fatalities_df: {fatalities_df['crash_id'].nunique()}\")\n",
    "print(f\"Total rows in fatalities_df: {len(fatalities_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94605a1-97c2-48a6-8ca0-87531c1ec65a",
   "metadata": {},
   "source": [
    "### Calculate missing ratio for geo fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f2813-06df-462e-b9db-9e2572ea120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing ratio for geo fields\n",
    "missing_rates = crashes_df[['national_lga_name_2021', 'sa4_name_2021']].isna().mean() * 100\n",
    "print(\"Missing Value Percentage:\")\n",
    "print(missing_rates)\n",
    "\n",
    "# Count 'Unknown' in national_remoteness_areas\n",
    "unknown_count = (crashes_df['national_remoteness_areas'] == 'Unknown').sum()\n",
    "total_rows = len(crashes_df)\n",
    "unknown_ratio = unknown_count / total_rows * 100\n",
    "print(f\"\\n'Unknown' in national_remoteness_areas: {unknown_count} ({unknown_ratio:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36542d4e-809d-45aa-9116-c21b6c900c4e",
   "metadata": {},
   "source": [
    "### Assessing Missing Data in Geographic Fields\n",
    "\n",
    "To evaluate whether geographic fields should be retained or excluded from the dataset, we applied a quantitative scoring method based on missingness, uniqueness, and correlation with a key target variable (`age`). The goal is to identify fields that offer high analytical value while minimizing noise and sparsity.\n",
    "\n",
    "We first cleaned the data by replacing known placeholders such as `'Unknown'` and `-9` with `NaN`, across both categorical and numerical fields. This ensured consistent handling of missing or invalid values prior to analysis.\n",
    "\n",
    "For each field, we calculated:\n",
    "- **Missing Rate**: proportion of missing values\n",
    "- **Uniqueness Score**: the number of unique values normalized by dataset size\n",
    "- **Correlation with Target**: absolute correlation with the target variable (`age`), where applicable\n",
    "\n",
    "A composite **Total Score** was then computed by weighting these factors (50% missingness, 20% uniqueness, 30% correlation). Fields with higher scores are considered more reliable and analytically valuable.\n",
    "\n",
    "The resulting ranking allows us to compare all fields fairly, including the three geographic fields of interest: `national_lga_name_2021`, `sa4_name_2021`, and `national_remoteness_areas`. By examining their scores in relation to other variables, we can make a more informed decision on whether to retain or remove them from the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ced37-7dcb-4ef5-81dd-800c22bd48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target variable for correlation analysis\n",
    "target_variable = 'age'\n",
    "\n",
    "# Create a copy of fatalities dataframe for analysis\n",
    "temp_df = fatalities_df.copy()\n",
    "\n",
    "# Define special values that should be treated as missing values\n",
    "special_values = {\n",
    "    'national_remoteness_areas': ['Unknown'],\n",
    "    'day_of_week': ['Unknown'],\n",
    "    'time_of_day': ['Unknown'],\n",
    "}\n",
    "\n",
    "# Replace special values with NaN\n",
    "for field, values in special_values.items():\n",
    "    if field in temp_df.columns:\n",
    "        for val in values:\n",
    "            temp_df.loc[temp_df[field] == val, field] = np.nan\n",
    "\n",
    "# Define fields where -9 should be treated as missing value\n",
    "neg9_fields = ['bus_involvement', 'heavy_rigid_truck_involvement', \n",
    "               'articulated_truck_involvement', 'speed_limit', 'gender', 'age', 'age_group']\n",
    "for field in neg9_fields:\n",
    "    if field in temp_df.columns:\n",
    "        temp_df.loc[(temp_df[field] == -9) | (temp_df[field] == '-9'), field] = np.nan\n",
    "\n",
    "# Get list of numerical columns for correlation analysis\n",
    "numerical_cols = temp_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Calculate field scores based on three metrics:\n",
    "# 1. Missing rate (lower is better)\n",
    "# 2. Unique values ratio (higher is better)\n",
    "# 3. Correlation with target variable (higher is better)\n",
    "field_scores = pd.DataFrame({\n",
    "    'Missing_Rate': temp_df.isna().mean(),\n",
    "    'Unique_Values': temp_df.nunique() / len(temp_df),\n",
    "})\n",
    "\n",
    "# Calculate correlations with target variable\n",
    "correlations = {}\n",
    "for col in temp_df.columns:\n",
    "    if col != target_variable and col in numerical_cols:\n",
    "        try:\n",
    "            corr = abs(temp_df[col].corr(temp_df[target_variable]))\n",
    "            correlations[col] = corr\n",
    "        except:\n",
    "            correlations[col] = 0\n",
    "    else:\n",
    "        correlations[col] = 0\n",
    "\n",
    "field_scores['Correlation_With_Target'] = pd.Series(correlations)\n",
    "\n",
    "# Calculate total score using weighted combination of metrics\n",
    "field_scores['Total_Score'] = (\n",
    "    (1 - field_scores['Missing_Rate']) * 0.5 +  # Missing rate weight: 50%\n",
    "    field_scores['Unique_Values'] * 0.2 +        # Unique values weight: 20%\n",
    "    field_scores['Correlation_With_Target'] * 0.3  # Correlation weight: 30%\n",
    ")\n",
    "\n",
    "# Sort fields by total score and get top 25\n",
    "field_scores = field_scores.sort_values('Total_Score', ascending=False)\n",
    "top_fields = field_scores.sort_values('Total_Score', ascending=False).head(25)\n",
    "\n",
    "# Create horizontal bar chart of top fields\n",
    "top_fields['Total_Score'].sort_values().plot(\n",
    "    kind='barh', \n",
    "    figsize=(10, 6), \n",
    "    title='Top 20 Fields by Total Score',\n",
    "    xlabel='Total Score',\n",
    "    color='skyblue',\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze scores for geographical fields\n",
    "geo_fields = ['national_lga_name_2021', 'sa4_name_2021', 'national_remoteness_areas']\n",
    "geo_scores = field_scores.loc[geo_fields]\n",
    "print(geo_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495e194-0371-4973-b9f0-cb8c7be8719f",
   "metadata": {},
   "source": [
    "### Decision on Dropping Geographic Fields\n",
    "\n",
    "To evaluate whether geographic fields should be retained in the dataset, we applied a multi-criteria scoring approach that considers missing rate, uniqueness, and correlation with a key outcome variable (`age`). This method assigns a composite score to each field, allowing us to assess its analytical value in a structured and consistent way.\n",
    "\n",
    "The resulting scores for the three geographic fields were:\n",
    "\n",
    "- `national_lga_name_2021`: Total Score = 0.113  \n",
    "- `sa4_name_2021`: Total Score = 0.112  \n",
    "- `national_remoteness_areas`: Total Score = 0.100  \n",
    "\n",
    "In contrast, most other fields in the dataset scored above 0.5, indicating significantly higher relevance and data quality.\n",
    "\n",
    "Given that:\n",
    "\n",
    "- These geographic fields have **very low total scores**, suggesting limited utility for predictive or descriptive analysis,\n",
    "- They exhibit **high proportions of missing or 'Unknown' values**, reducing their reliability,\n",
    "- They are **not directly aligned with the current analytical focus on victim-level characteristics**,\n",
    "\n",
    "We conclude that it is both justified and beneficial to **remove these fields** to simplify the dataset and improve overall signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246338d-84d4-4408-b4dc-91db8fe115f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 3 original geographic fields\n",
    "columns_to_drop = [\n",
    "    'national_lga_name_2021',\n",
    "    'sa4_name_2021',\n",
    "    'national_remoteness_areas'\n",
    "]\n",
    "crashes_df.drop(columns=columns_to_drop,inplace=True)\n",
    "fatalities_df.drop(columns=columns_to_drop, inplace=True)\n",
    "crashes_df.info();\n",
    "fatalities_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48a606-23c7-4b6f-9ace-22250e49e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_df retains fields required for feature engineering\n",
    "columns_to_keep = ['state', 'lga_code', 'lga_name', 'persons_no', 'median_age', 'pct_65_plus']\n",
    "age_df_cleaned = age_df[columns_to_keep].copy()\n",
    "\n",
    "age_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289f2b9-adc8-4940-847b-274cfb321c18",
   "metadata": {},
   "source": [
    "### Convert special missing value indicators (-9, \"Unknown\", etc.) to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70f9e2-da63-440e-be03-0a36f009fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_neg9_columns(df: pd.DataFrame, df_name: str = \"DataFrame\") -> list:\n",
    "    \"\"\"\n",
    "    Detect which columns in a DataFrame contain the value -9 (int or str).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        df_name (str): Optional name of the DataFrame (for print info)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Columns containing -9\n",
    "    \"\"\"\n",
    "    columns_with_neg9 = []\n",
    "\n",
    "    print(f\"Scanning for '-9' in {df_name}...\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_str = df[col].astype(str).str.strip()\n",
    "        if (col_str == '-9').any():\n",
    "            count = (col_str == '-9').sum()\n",
    "            columns_with_neg9.append(col)\n",
    "            print(f\"‚úÖ Column '{col}' contains -9 ‚Üí {count} rows\")\n",
    "\n",
    "    if not columns_with_neg9:\n",
    "        print(\"‚úÖ No columns contain -9.\")\n",
    "    else:\n",
    "        print(f\"üìå Columns with '-9' in {df_name}: {columns_with_neg9}\")\n",
    "\n",
    "    return columns_with_neg9\n",
    "\n",
    "crashes_neg9_cols = detect_neg9_columns(crashes_df, df_name=\"crashes_df\")\n",
    "fatalities_neg9_cols = detect_neg9_columns(fatalities_df, df_name=\"fatalities_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc42cd-9a69-47b6-b6ba-040323fd6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert -9 and \"-9\" to NaN for both datasets\n",
    "for col in crashes_neg9_cols:\n",
    "    crashes_df.loc[crashes_df[col].isin(['-9', -9]), col] = np.nan\n",
    "\n",
    "for col in fatalities_neg9_cols:\n",
    "    fatalities_df.loc[fatalities_df[col].isin(['-9', -9]), col] = np.nan\n",
    "\n",
    "# Special handling for road_user column\n",
    "mask = fatalities_df['road_user'].str.contains('Other/-9', na=False)\n",
    "fatalities_df.loc[mask, 'road_user'] = np.nan\n",
    "\n",
    "# Standardize various forms of \"Unknown\" to NaN\n",
    "unknown_values = ['unknown', 'Unknown', 'UNKNOWN']\n",
    "crashes_df['day_of_week'] = crashes_df['day_of_week'].replace(unknown_values, np.nan)\n",
    "crashes_df['time_of_day'] = crashes_df['time_of_day'].replace(unknown_values, np.nan)\n",
    "fatalities_df['road_user'] = fatalities_df['road_user'].replace(unknown_values, np.nan)\n",
    "\n",
    "# Create cleaned copies of the DataFrames\n",
    "crashes_df_cleaned = crashes_df.copy()\n",
    "fatalities_df_cleaned = fatalities_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcf5c4-7f16-4ddf-967b-3dd047d4cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1Ô∏è‚É£: Define key fields for missing value analysis\n",
    "key_fields = [\n",
    "    'bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement',\n",
    "    'speed_limit', 'gender', 'age', 'age_group', 'road_user'\n",
    "]\n",
    "\n",
    "# Step 2Ô∏è‚É£: Calculate number of missing values per row\n",
    "fatalities_df_cleaned['num_missing'] = fatalities_df_cleaned[key_fields].isna().sum(axis=1)\n",
    "\n",
    "# Step 3Ô∏è‚É£: Identify records with ‚â• threshold missing fields\n",
    "threshold = 4\n",
    "many_missing_rows = fatalities_df_cleaned[fatalities_df_cleaned['num_missing'] >= threshold]\n",
    "target_crash_ids = many_missing_rows['crash_id'].unique()\n",
    "print(f\"ID Affected crash_id count: {len(target_crash_ids)}\")\n",
    "\n",
    "# Step 4Ô∏è‚É£: Remove identified records from both datasets\n",
    "before_crash = len(crashes_df_cleaned)\n",
    "crashes_df_cleaned = crashes_df_cleaned[~crashes_df_cleaned['crash_id'].isin(target_crash_ids)]\n",
    "after_crash = len(crashes_df_cleaned)\n",
    "print(\"crash_id values to be deleted:\")\n",
    "print(target_crash_ids)\n",
    "\n",
    "before_fatalities = len(fatalities_df_cleaned)\n",
    "fatalities_df_cleaned = fatalities_df_cleaned[~fatalities_df_cleaned['crash_id'].isin(target_crash_ids)]\n",
    "after_fatalities = len(fatalities_df_cleaned)\n",
    "\n",
    "print(f\"Removed {before_crash - after_crash} rows from crashes_df_cleaned.\")\n",
    "print(f\"Removed {before_fatalities - after_fatalities} rows from fatalities_df_cleaned.\")\n",
    "\n",
    "# Clean up: Remove temporary column\n",
    "fatalities_df_cleaned.drop(columns=['num_missing'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995836f-7bbb-4938-a358-34b8e546556b",
   "metadata": {},
   "source": [
    "#### Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fad4a-f6d5-422e-b955-01368534b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to category type\n",
    "categorical_cols = [\n",
    "    'state', 'dayweek', 'crash_type', 'day_of_week', 'time_of_day',\n",
    "    'national_road_type'\n",
    "]\n",
    "categorical_cols_fatalities = ['road_user', 'gender', 'age_group']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    crashes_df_cleaned[col] = crashes_df_cleaned[col].astype('category')\n",
    "    \n",
    "for col in categorical_cols_fatalities:\n",
    "    fatalities_df_cleaned[col] = fatalities_df_cleaned[col].astype('category')\n",
    "\n",
    "# Convert binary columns to boolean type\n",
    "binary_cols = [\n",
    "    'bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement',\n",
    "    'christmas_period', 'easter_period'\n",
    "]\n",
    "\n",
    "for col in binary_cols:\n",
    "    crashes_df_cleaned[col] = (\n",
    "        crashes_df_cleaned[col].astype(str).str.lower().map({'yes': True, 'no': False}).astype('boolean')\n",
    "    )\n",
    "\n",
    "# Convert speed_limit to numeric type, handling special cases\n",
    "crashes_df_cleaned['speed_limit'] = crashes_df_cleaned['speed_limit'].replace('<40', 40)\n",
    "crashes_df_cleaned['speed_limit'] = (pd.to_numeric(crashes_df_cleaned['speed_limit'], errors='coerce').astype('Int64'))\n",
    "\n",
    "categorical_cols_fatalities = ['road_user', 'gender', 'age_group']\n",
    "for col in categorical_cols_fatalities:\n",
    "    fatalities_df_cleaned[col] = fatalities_df_cleaned[col].astype('category')\n",
    "\n",
    "\n",
    "# age_df_cleaned\n",
    "# Convert numeric columns in age profile dataset to proper numeric types\n",
    "# Using 'coerce' to handle any invalid values by converting them to NaN\n",
    "age_df_cleaned['persons_no'] = pd.to_numeric(age_df_cleaned['persons_no'], errors='coerce')\n",
    "age_df_cleaned['median_age'] = pd.to_numeric(age_df_cleaned['median_age'], errors='coerce')\n",
    "age_df_cleaned['pct_65_plus'] = pd.to_numeric(age_df_cleaned['pct_65_plus'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951429e-19d9-4f1b-9afb-e85cab429a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÁâπÂæÅÂ∑•Á®ã roll up/ drill down\n",
    "# ÂàõÂª∫Â≠£ËäÇ\n",
    "season_map = {12: 'Summer', 1: 'Summer', 2: 'Summer', \n",
    "              3: 'Autumn', 4: 'Autumn', 5: 'Autumn',\n",
    "              6: 'Winter', 7: 'Winter', 8: 'Winter', \n",
    "              9: 'Spring', 10: 'Spring', 11: 'Spring'}\n",
    "\n",
    "crashes_df_cleaned['season'] = crashes_df_cleaned['month'].map(season_map)\n",
    "crashes_df_cleaned['season'] = crashes_df_cleaned['season'].astype('category')\n",
    "\n",
    "\n",
    "hour_bin_labels = {\n",
    "    0: '00:00‚Äì02:00', 1: '02:00‚Äì04:00', 2: '04:00‚Äì06:00',\n",
    "    3: '06:00‚Äì08:00', 4: '08:00‚Äì10:00', 5: '10:00‚Äì12:00',\n",
    "    6: '12:00‚Äì14:00', 7: '14:00‚Äì16:00', 8: '16:00‚Äì18:00',\n",
    "    9: '18:00‚Äì20:00', 10: '20:00‚Äì22:00', 11: '22:00‚Äì00:00'\n",
    "}\n",
    "\n",
    "# ‰ªé time Â≠óÊÆµÁõ¥Êé•ÊèêÂèñÊó∂ÊÆµÊ†áÁ≠æ ‚Üí ÂÜôÂÖ• time_bin\n",
    "crashes_df_cleaned['time_bin'] = (\n",
    "    pd.to_numeric(\n",
    "        crashes_df_cleaned['time'].astype(str).str.extract(r'^(\\d{1,2})')[0],\n",
    "        errors='coerce'\n",
    "    ) // 2\n",
    ").map(hour_bin_labels).astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b89755-e3b1-4427-ab5c-00ce8c32e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df_cleaned.head(5)\n",
    "age_df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f362795-5108-46da-9d54-33900c3cc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map full state names to abbreviations\n",
    "state_abbrev_map = {\n",
    "    'New South Wales': 'NSW',\n",
    "    'Victoria': 'VIC',\n",
    "    'Queensland': 'QLD',\n",
    "    'South Australia': 'SA',\n",
    "    'Western Australia': 'WA',\n",
    "    'Tasmania': 'TAS',\n",
    "    'Northern Territory': 'NT',\n",
    "    'Australian Capital Territory': 'ACT',\n",
    "    'Other Territories': 'OT'\n",
    "}\n",
    "age_df_cleaned['state'] = age_df_cleaned['state'].replace(state_abbrev_map)\n",
    "\n",
    "# Calculate state-level weighted averages\n",
    "age_df_cleaned = age_df_cleaned.groupby('state').apply(\n",
    "    lambda g: pd.Series({\n",
    "        'total_population': g['persons_no'].sum(),\n",
    "        'weighted_median_age': round((g['median_age'] * g['persons_no']).sum() / g['persons_no'].sum(), 1),\n",
    "        'weighted_pct_65_plus': round((g['pct_65_plus'] * g['persons_no']).sum() / g['persons_no'].sum(), 1)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Define classification functions for population structure\n",
    "def classify_population_structure(median_age, pct_65):\n",
    "    if median_age < 30 and pct_65 < 10:\n",
    "        return 'Young society'\n",
    "    elif 30 <= median_age < 40 and 10 <= pct_65 < 14:\n",
    "        return 'Mature society'\n",
    "    elif 40 <= median_age <= 45 and 14 <= pct_65 < 22:\n",
    "        return 'Aging society'\n",
    "    elif median_age > 45 and pct_65 >= 22:\n",
    "        return 'Super-aged society'\n",
    "    else:\n",
    "        return 'Transitional society'\n",
    "        \n",
    "def classify_abs_age_group(pct):\n",
    "    if pct < 10.0:\n",
    "        return 'Less than 10%'\n",
    "    elif pct < 14.0:\n",
    "        return '10% to <14%'\n",
    "    elif pct < 18.0:\n",
    "        return '14% to <18%'\n",
    "    elif pct < 22.0:\n",
    "        return '18% to <22%'\n",
    "    else:\n",
    "        return '22% or more'\n",
    "        \n",
    "# Apply classifications to create new features\n",
    "age_df_cleaned['population_structure_2023'] = age_df_cleaned.apply(\n",
    "    lambda row: classify_population_structure(row['weighted_median_age'], row['weighted_pct_65_plus']),\n",
    "    axis=1\n",
    ")\n",
    "age_df_cleaned['abs_pct_65_plus_group_2023'] = age_df_cleaned['weighted_pct_65_plus'].apply(classify_abs_age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88e542-5034-4324-9d02-89341d4ba42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1506c-428f-4ee5-b43f-238a5877cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate actual fatality counts per crash from fatalities dataset\n",
    "fatality_counts = fatalities_df_cleaned.groupby('crash_id').size()\n",
    "\n",
    "# Identify crashes where reported fatalities don't match actual records\n",
    "crashes_with_mismatch = crashes_df_cleaned[\n",
    "    crashes_df_cleaned['crash_id'].isin(fatality_counts.index) & \n",
    "    (crashes_df_cleaned['number_fatalities'] != fatality_counts[crashes_df_cleaned['crash_id']].values)\n",
    "]\n",
    "\n",
    "# Report number of inconsistencies found\n",
    "print(f\"Crashes with fatality count mismatch: {len(crashes_with_mismatch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359d35a-15a3-4b50-afe0-2120375d938e",
   "metadata": {},
   "source": [
    "### Data Integration: Fact Table Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f90785-c915-42ee-ac4b-819e5b84e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify duplicate columns (excluding crash_id)\n",
    "common_columns = set(fatalities_df_cleaned.columns).intersection(crashes_df_cleaned.columns)\n",
    "common_columns.discard('crash_id')  # Preserve primary key\n",
    "\n",
    "# Step 2: Remove duplicate columns from fatalities dataset\n",
    "fatalities_reduced = fatalities_df_cleaned.drop(columns=common_columns)\n",
    "fatalities_reduced.head(5)\n",
    "\n",
    "# Step 3: Merge datasets without column conflicts\n",
    "fact_df = fatalities_reduced.merge(crashes_df_cleaned, on='crash_id', how='left')\n",
    "\n",
    "# Add fatality_id as primary key\n",
    "fact_df = fact_df.reset_index(drop=True)\n",
    "fact_df.insert(0, 'fatality_id', fact_df.index + 1)\n",
    "\n",
    "# Standardize state names and merge age profile data\n",
    "fact_df['state'] = fact_df['state'].str.upper()\n",
    "age_df_cleaned['state'] = age_df_cleaned['state'].str.upper()\n",
    "\n",
    "fact_df = fact_df.merge(\n",
    "    age_df_cleaned[['state','population_structure_2023', 'abs_pct_65_plus_group_2023']],\n",
    "    on='state',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Export the prepared of Association Rule Mining\n",
    "fact_df.to_csv('fatalities_mining_dataset.csv', index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4b7d2-addf-49c7-bc3c-10597be226d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad753be8-c5e6-484c-abac-7464f9d431f3",
   "metadata": {},
   "source": [
    "### Dimensional Modeling: Dimension Table Creation and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf8bd5-33f2-4fa9-a280-1651d8543c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_dimension_key(fact_df, dim_df, dim_cols, key_name):\n",
    "    \"\"\"\n",
    "    Replace dimension attributes in fact_df with their corresponding dimension key\n",
    "    and remove the original attributes\n",
    "    \"\"\"\n",
    "    fact_df = fact_df.merge(dim_df, on=dim_cols, how='left')\n",
    "    fact_df = fact_df.drop(columns=dim_cols)\n",
    "    return fact_df\n",
    "\n",
    "# Create and populate dimension tables\n",
    "# 1Ô∏è‚É£ Dim_Time: Temporal dimension (year, month, season)\n",
    "dim_time = (\n",
    "    fact_df[['year', 'month', 'season']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(['year', 'month'])\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'time_id'})\n",
    ")\n",
    "dim_time['time_id'] += 1\n",
    "dim_time.to_csv('dim_time.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_time, ['year', 'month', 'season'], 'time_id')\n",
    "\n",
    "# 2Ô∏è‚É£ Dim_Date: Date-related attributes\n",
    "dim_date = (\n",
    "    fact_df[['dayweek', 'day_of_week']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'date_id'})\n",
    ")\n",
    "dim_date['date_id'] += 1\n",
    "dim_date.to_csv('dim_date.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_date, ['dayweek', 'day_of_week'], 'date_id')\n",
    "\n",
    "# 3Ô∏è‚É£ Dim_DayNight: Time of day attributes\n",
    "dim_daynight = (\n",
    "    fact_df[['time', 'time_bin', 'time_of_day']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'time_of_day_id'})\n",
    ")\n",
    "dim_daynight['time_of_day_id'] += 1\n",
    "dim_daynight.to_csv('dim_daynight.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_daynight, ['time', 'time_bin', 'time_of_day'], 'time_of_day_id')\n",
    "\n",
    "# 4Ô∏è‚É£ Dim_VehicleInvl: Vehicle involvement attributes\n",
    "dim_vehicle_invl = (\n",
    "    fact_df[['bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'vehicle_invl_id'})\n",
    ")\n",
    "dim_vehicle_invl['vehicle_invl_id'] += 1\n",
    "dim_vehicle_invl.to_csv('dim_vehicle_invl.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(\n",
    "    fact_df,\n",
    "    dim_vehicle_invl,\n",
    "    ['bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement'],\n",
    "    'vehicle_invl_id'\n",
    ")\n",
    "\n",
    "# 5Ô∏è‚É£ Dim_Crash: Crash type attributes\n",
    "dim_crash = (\n",
    "    fact_df[['crash_id', 'crash_type']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'crash_dim_id'})\n",
    ")\n",
    "dim_crash['crash_dim_id'] += 1\n",
    "dim_crash.to_csv('dim_crash.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_crash[['crash_id', 'crash_type', 'crash_dim_id']], ['crash_id', 'crash_type'], 'crash_dim_id')\n",
    "\n",
    "# 6Ô∏è‚É£ Dim_Holiday: Holiday period attributes\n",
    "dim_holiday = (\n",
    "    fact_df[['christmas_period', 'easter_period']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'holiday_id'})\n",
    ")\n",
    "dim_holiday['holiday_id'] += 1\n",
    "dim_holiday.to_csv('dim_holiday.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_holiday, ['christmas_period', 'easter_period'], 'holiday_id')\n",
    "\n",
    "# 7Ô∏è‚É£ Dim_Road: Road-related attributes\n",
    "dim_road = (\n",
    "    fact_df[['speed_limit', 'national_road_type']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'road_id'})\n",
    ")\n",
    "dim_road['road_id'] += 1\n",
    "dim_road.to_csv('dim_road.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_road, ['speed_limit', 'national_road_type'], 'road_id')\n",
    "\n",
    "# 8Ô∏è‚É£ Dim_State_Aging_Level: Population aging attributes\n",
    "dim_state_aging_level = (\n",
    "    fact_df[['state', 'population_structure_2023', 'abs_pct_65_plus_group_2023']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'index': 'population_age_id',\n",
    "        'population_structure_2023': 'population_structure_2023',\n",
    "        'abs_pct_65_plus_group_2023': 'abs_pct_65_plus_group_2023'\n",
    "    })\n",
    ")\n",
    "dim_state_aging_level['population_age_id'] += 1\n",
    "dim_state_aging_level.to_csv('dim_state_aging_level.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(\n",
    "    fact_df,\n",
    "    dim_state_aging_level,\n",
    "    ['state', 'population_structure_2023', 'abs_pct_65_plus_group_2023'],\n",
    "    'population_age_id'\n",
    ")\n",
    "\n",
    "# 9Ô∏è‚É£ Dim_Victim: Victim-related attributes\n",
    "dim_victim = (\n",
    "    fact_df[['gender', 'age_group', 'road_user']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'victim_type_id'})\n",
    ")\n",
    "dim_victim['victim_type_id'] += 1\n",
    "dim_victim.to_csv('dim_victim.csv', index=False)\n",
    "fact_df = replace_with_dimension_key(fact_df, dim_victim, ['gender', 'age_group', 'road_user'], 'victim_type_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2957762-0ad1-4886-b787-0c6642410308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age-based measures as binary indicators\n",
    "fact_df['is_young'] = fact_df['age'].between(18, 24, inclusive='both').astype(int)\n",
    "fact_df['is_general'] = fact_df['age'].between(25, 64, inclusive='both').astype(int)\n",
    "fact_df['is_senior'] = (fact_df['age'] >= 65).astype(int)\n",
    "\n",
    "# Remove number_fatalities as it's redundant in our fact table design\n",
    "# Since our fact table is victim-centric (one row per victim), \n",
    "# each row represents a single fatality, making number_fatalities unnecessary\n",
    "fact_df = fact_df.drop(columns=['number_fatalities'])\n",
    "\n",
    "# Export final fact table for PostgreSQL import\n",
    "fact_df.to_csv('fact_fatalities.csv', index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba0920-0519-461f-be17-076b38d7d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"END\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
